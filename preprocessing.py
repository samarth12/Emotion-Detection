import numpy as np
import re
import itertools
from collections import Counter
import csv

def clean_str(string):
    string = re.sub(r"[^A-Za-z0-9(),!?\'\`]", " ", string)
    string = re.sub(r"\'s", " \'s", string)
    string = re.sub(r"\'ve", " \'ve", string)
    string = re.sub(r"n\'t", " n\'t", string)
    string = re.sub(r"\'re", " \'re", string)
    string = re.sub(r"\'d", " \'d", string)
    string = re.sub(r"\'ll", " \'ll", string)
    string = re.sub(r",", " , ", string)
    string = re.sub(r"!", " ! ", string)
    string = re.sub(r"\(", " \( ", string)
    string = re.sub(r"\)", " \) ", string)
    string = re.sub(r"\?", " \? ", string)
    string = re.sub(r"\s{2,}", " ", string)
    return string.strip().lower()


def load_data(fname):
    file_reader= open(fname, "rt")
    read = csv.reader(file_reader)
    data = []
    for row in read :
        x_text = [clean_str(sent) for sent in row]
        data.append(x_text)


    #print(data)
    #x_text = [clean_string(sent) for sent in data]
    x = [x_text[0] for x_text in data]
    y = [x_text[1] for x_text in data]
        #print(y)
    #print(x)
    #print(x)
    all_label = dict()
    for label in x:
        if not label in all_label:
            all_label[label] = len(all_label) + 1

    l = list(all_label.keys())
    l.sort()
    df = len(y)
    print(all_label)
    one_hot = np.identity(len(all_label))
    x = [one_hot[ all_label[label]-1 ] for label in x]
    x = [l.tolist() for l in x]
    x = np.array(x)

    vocabulary, vocabulary_inv = build_vocab(y)
    word2vec = vocab_to_word2vec("GoogleNews-vectors-negative300.bin", vocabulary)
    embedding_mat = build_word_embedding_mat(word2vec, vocabulary_inv)

    return y, x, df, l, embedding_mat

def build_vocab(sentences):

    # Build vocabulary
    word_counts = Counter(itertools.chain(*sentences))
    # Mapping from index to word
    vocabulary_inv = [x[0] for x in word_counts.most_common()]
    print(vocabulary_inv)
    # Mapping from word to index
    vocabulary = {x: i + 1 for i, x in enumerate(vocabulary_inv)}
    return [vocabulary, vocabulary_inv]

def build_word_embedding_mat(word_vecs, vocabulary_inv, k=300):

    vocab_size = len(vocabulary_inv)
    embedding_mat = np.zeros(shape=(9017, k), dtype='float32')
    for idx in range(len(vocabulary_inv)):
        embedding_mat[idx + 1] = word_vecs[vocabulary_inv[idx]]
    print "Embedding matrix of size " + str(np.shape(embedding_mat))
    # initialize the first row,
    embedding_mat[0] = np.random.uniform(-0.25, 0.25, k)
    return embedding_mat

def vocab_to_word2vec(fname, vocab, k=300):

    word_vecs = {}
    with open(fname, "rb") as f:
        header = f.readline()
        vocab_size, layer1_size = map(int, header.split())
        binary_len = np.dtype('float32').itemsize * layer1_size
        for line in xrange(vocab_size):
            word = []
            while True:
                ch = f.read(1)
                if ch == ' ':
                    word = ''.join(word)
                    break
                if ch != '\n':
                    word.append(ch)
            if word in vocab:
                word_vecs[word] = np.fromstring(f.read(binary_len), dtype='float32')
            else:
                f.read(binary_len)
    print str(len(word_vecs)) + " words found in word2vec."

    # add unknown words by generating random word vectors
    count_missing = 0
    for word in vocab:
        if word not in word_vecs:
            word_vecs[word] = np.random.uniform(-0.25, 0.25, k)
            count_missing += 1
    print str(count_missing) + " words not found, generated by random."
    return word_vecs


def build_input_data(sentences, labels, vocabulary):
    x = [[vocabulary[word] for word in sentence] for sentence in sentences]
    y = np.array(labels)
    print(vocabulary)
    #print(labels)
    #print(x)
    #print(y)
    return [x, y]

def load_embedding_vectors(vocabulary):
    # load embedding_vectors from the word2vec
    filename = 'GoogleNews-vectors-negative300.bin'
    encoding = 'utf-8'
    with open(filename, "rb") as f:
        header = f.readline()
        vocab_size, vector_size = map(int, header.split())
        # initial matrix with random uniform
        embedding_vectors = np.random.uniform(-0.25, 0.25, (len(vocabulary), vector_size))
        if True:
            binary_len = np.dtype('float32').itemsize * vector_size
            for line_no in range(vocab_size):
                word = []
                while True:
                    ch = f.read(1)
                    if ch == b' ':
                        break
                    if ch == b'':
                        raise EOFError("unexpected end of input; is count incorrect or file otherwise damaged?")
                    if ch != b'\n':
                        word.append(ch)
                word = str(b''.join(word))
                idx = vocabulary.get(word)
                if idx != 0:
                    embedding_vectors[idx] = np.fromstring(f.read(binary_len), dtype='float32')
                else:

                    f.seek(binary_len, 1)
        #a = np.delete(embedding_vectors, slice(8999,9016), axis =0)
        #print(a)

        f.close()
        return embedding_vectors

def generate_batches(data, batch_size, num_epochs, shuffle=True):

    data = np.array(data)
    data_size = len(data)
    num_batches_per_epoch = int((len(data)-1)/batch_size) + 1
    for epoch in range(num_epochs):
        # Shuffle the data at each epoch
        if shuffle:
            shuffle_indices = np.random.permutation(np.arange(data_size))
            shuffled_data = data[shuffle_indices]
        else:
            shuffled_data = data
        for batch_num in range(num_batches_per_epoch):
            start_index = batch_num * batch_size
            end_index = min((batch_num + 1) * batch_size, data_size)
            yield shuffled_data[start_index:end_index]
